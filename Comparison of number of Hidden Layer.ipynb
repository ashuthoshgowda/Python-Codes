{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Data set\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "N = 100 # number of points per class\n",
    "D = 2 # dimensionality\n",
    "K = 3 # number of classes\n",
    "X = np.zeros((N*K,D))\n",
    "y = np.zeros(N*K, dtype='uint8')\n",
    "for j in range(K):\n",
    "  ix = range(N*j,N*(j+1))\n",
    "  r = np.linspace(0.0,1,N) # radius\n",
    "  t = np.linspace(j*4,(j+1)*4,N) + np.random.randn(N)*0.2 # theta\n",
    "  X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]\n",
    "  y[ix] = j\n",
    "\n",
    "pickle.dump(X,open('dataX.pickle','wb'))\n",
    "pickle.dump(y,open('dataY.pickle','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Training Dataset X Shape :  (240, 2)\n",
      " Test Dataset X Shape :  (60, 2)\n",
      " Training Dataset Y Shape :  (240,)\n",
      " Test Dataset Y Shape :  (60,)\n"
     ]
    }
   ],
   "source": [
    "#Partition\n",
    "from sklearn.model_selection import train_test_split\n",
    "data_X_train, data_X_test, data_Y_train, data_Y_test = train_test_split(X, y, test_size=0.20)\n",
    "print (\" Training Dataset X Shape : \",data_X_train.shape)\n",
    "print (\" Test Dataset X Shape : \",data_X_test.shape)\n",
    "print (\" Training Dataset Y Shape : \",data_Y_train.shape)\n",
    "print (\" Test Dataset Y Shape : \",data_Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ffn(X,y,s,r,g_iter,h1):\n",
    "    #For simplicity we will take the batch size to be the same as number of examples\n",
    "    num_examples = X.shape[0]\n",
    "    h = h1\n",
    "    np.random.seed(1234)\n",
    "    W = 0.01 * np.random.randn(D,h)\n",
    "    b = np.zeros((1,h))\n",
    "    W2 = 0.01 * np.random.randn(h,K)\n",
    "    b2 = np.zeros((1,K))\n",
    "    \n",
    "    #Initial value for the Gradient Descent Parameter\n",
    "    step_size = s #Also called learning rate\n",
    "\n",
    "    #For simplicity, we will not hand tune this algorithm parameter as well.\n",
    "    reg= r\n",
    "    \n",
    "    # gradient descent loop\n",
    "    for i in range(g_iter):\n",
    "      \n",
    "  # evaluate class scores, [N x K]\n",
    "        hidden_layer = np.maximum(0, np.dot(X, W) + b) # note, ReLU activation\n",
    "        scores = np.dot(hidden_layer, W2) + b2\n",
    "  \n",
    "  # compute the class probabilities\n",
    "        exp_scores = np.exp(scores)\n",
    "        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True) # [N x K]\n",
    "  \n",
    "  # compute the loss: average cross-entropy loss and regularization\n",
    "        corect_logprobs = -np.log(probs[range(num_examples),y])\n",
    "        data_loss = np.sum(corect_logprobs)/num_examples\n",
    "        reg_loss = 0.5*reg*np.sum(W*W) + 0.5*reg*np.sum(W2*W2)\n",
    "        loss = data_loss + reg_loss\n",
    "        #if i % 1000 == 0:\n",
    "            #print \"iteration %d: loss %f\" % (i, loss)\n",
    "  \n",
    "  # compute the gradient on scores\n",
    "        dscores = probs\n",
    "        dscores[range(num_examples),y] -= 1\n",
    "        dscores /= num_examples\n",
    "  \n",
    "  # backpropate the gradient to the parameters\n",
    "  # first backprop into parameters W2 and b2\n",
    "        dW2 = np.dot(hidden_layer.T, dscores)\n",
    "        db2 = np.sum(dscores, axis=0, keepdims=True)\n",
    "  # next backprop into hidden layer\n",
    "        dhidden = np.dot(dscores, W2.T)\n",
    "  # backprop the ReLU non-linearity\n",
    "        dhidden[hidden_layer <= 0] = 0\n",
    "  # finally into W,b\n",
    "        dW = np.dot(X.T, dhidden)\n",
    "        db = np.sum(dhidden, axis=0, keepdims=True)\n",
    "  \n",
    "  # add regularization gradient contribution\n",
    "        dW2 += reg * W2\n",
    "        dW += reg * W\n",
    "  \n",
    "  # perform a parameter update\n",
    "        W += -step_size * dW\n",
    "        b += -step_size * db\n",
    "        W2 += -step_size * dW2\n",
    "        b2 += -step_size * db2\n",
    "        \n",
    "    return(W,b,W2,b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Hidden Units: 50.00\n",
      "train accuracy : 65.83\n",
      "Test accuracy: 58.33\n",
      "Number of Hidden Units: 100.00\n",
      "train accuracy : 67.92\n",
      "Test accuracy: 61.67\n",
      "Number of Hidden Units: 150.00\n",
      "train accuracy : 69.58\n",
      "Test accuracy: 61.67\n"
     ]
    }
   ],
   "source": [
    "#Question 5.4.1\n",
    "giter=[50,100,150]\n",
    "for j in giter[:]:\n",
    "    W,b,W2,b2= ffn(data_X_train,data_Y_train,0.4,0.001,500,j)\n",
    "\n",
    "    hidden_layer = np.maximum(0, np.dot(data_X_train, W) + b)\n",
    "    scores = np.dot(hidden_layer, W2) + b2\n",
    "    predicted_class = np.argmax(scores, axis=1)\n",
    "    print('Number of Hidden Units: %.2f' % j)\n",
    "    print ('train accuracy : %.2f' % (np.mean(predicted_class == data_Y_train)*100))\n",
    "\n",
    "    hidden_layer = np.maximum(0, np.dot(data_X_test, W) + b)\n",
    "    scores = np.dot(hidden_layer, W2) + b2\n",
    "    predicted_class = np.argmax(scores, axis=1)\n",
    "    Test_Acc=(np.mean(predicted_class == data_Y_test)*100)\n",
    "    print ('Test accuracy: %.2f' % Test_Acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ffnl(X,y,s,r,g_iter,h):\n",
    "    #For simplicity we will take the batch size to be the same as number of examples\n",
    "    num_examples = X.shape[0]\n",
    "    h = h\n",
    "    np.random.seed(1234)\n",
    "    W = 0.01 * np.random.randn(D,h)\n",
    "    b = np.zeros((1,h))\n",
    "    W2 = 0.01 * np.random.randn(h,K)\n",
    "    b2 = np.zeros((1,K))\n",
    "    \n",
    "    #Initial value for the Gradient Descent Parameter\n",
    "    step_size = s #Also called learning rate\n",
    "\n",
    "    #For simplicity, we will not hand tune this algorithm parameter as well.\n",
    "    reg= r\n",
    "    \n",
    "    # gradient descent loop\n",
    "    for i in range(g_iter):\n",
    "      \n",
    "  # evaluate class scores, [N x K]\n",
    "        #hidden_layer = np.maximum(0.01*x, np.dot(X, W) + b)# note, ReLU activation\n",
    "        hidden_layer = np.maximum(np.dot(0.01*X,W)+b  , np.dot(X,W)+ b) # Leaky ReLU activation\n",
    "        scores = np.dot(hidden_layer, W2) + b2\n",
    "         \n",
    "  # compute the class probabilities\n",
    "        exp_scores = np.exp(scores)\n",
    "        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True) # [N x K]\n",
    "  \n",
    "  # compute the loss: average cross-entropy loss and regularization\n",
    "        corect_logprobs = -np.log(probs[range(num_examples),y])\n",
    "        data_loss = np.sum(corect_logprobs)/num_examples\n",
    "        reg_loss = 0.5*reg*np.sum(W*W) + 0.5*reg*np.sum(W2*W2)\n",
    "        loss = data_loss + reg_loss\n",
    "        #if i % 1000 == 0:\n",
    "            #print \"iteration %d: loss %f\" % (i, loss)\n",
    "  \n",
    "  # compute the gradient on scores\n",
    "        dscores = probs\n",
    "        dscores[range(num_examples),y] -= 1\n",
    "        dscores /= num_examples\n",
    "  \n",
    "  # backpropate the gradient to the parameters\n",
    "  # first backprop into parameters W2 and b2\n",
    "        dW2 = np.dot(hidden_layer.T, dscores)\n",
    "        db2 = np.sum(dscores, axis=0, keepdims=True)\n",
    "  # next backprop into hidden layer\n",
    "        dhidden = np.dot(dscores, W2.T)\n",
    "  # backprop the ReLU non-linearity\n",
    "        dhidden[hidden_layer <= 0] *= 0.01\n",
    "  # finally into W,b\n",
    "        dW = np.dot(X.T, dhidden)\n",
    "        db = np.sum(dhidden, axis=0, keepdims=True)\n",
    "  \n",
    "  # add regularization gradient contribution\n",
    "        dW2 += reg * W2\n",
    "        dW += reg * W\n",
    "  \n",
    "  # perform a parameter update\n",
    "        W += -step_size * dW\n",
    "        b += -step_size * db\n",
    "        W2 += -step_size * dW2\n",
    "        b2 += -step_size * db2\n",
    "        \n",
    "    return(W,b,W2,b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Hidden Units: 50.00\n",
      "train accuracy : 55.42\n",
      "Test accuracy: 55.00\n",
      "Number of Hidden Units: 100.00\n",
      "train accuracy : 56.25\n",
      "Test accuracy: 55.00\n",
      "Number of Hidden Units: 150.00\n",
      "train accuracy : 54.17\n",
      "Test accuracy: 53.33\n"
     ]
    }
   ],
   "source": [
    "#Question 5.4.2\n",
    "giter=[50,100,150]\n",
    "for j in giter[:]:\n",
    "    W,b,W2,b2= ffnl(data_X_train,data_Y_train,0.4,0.001,1000,j)\n",
    "\n",
    "    hidden_layer = np.maximum(np.dot(0.01*data_X_train,W)+b  , np.dot(data_X_train,W)+ b)\n",
    "    scores = np.dot(hidden_layer, W2) + b2\n",
    "    predicted_class = np.argmax(scores, axis=1)\n",
    "    print('Number of Hidden Units: %.2f' % j)\n",
    "    print ('train accuracy : %.2f' % (np.mean(predicted_class == data_Y_train)*100))\n",
    "\n",
    "    hidden_layer = np.maximum(np.dot(0.01*data_X_test,W)+b  , np.dot(data_X_test,W)+ b)\n",
    "    scores = np.dot(hidden_layer, W2) + b2\n",
    "    predicted_class = np.argmax(scores, axis=1)\n",
    "    Test_Acc=(np.mean(predicted_class == data_Y_test)*100)\n",
    "    print ('Test accuracy: %.2f' % Test_Acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ffnmo(X,y,step_size, reg, no_iter,h):\n",
    "        \n",
    "    # Start with an initial set of parameters randomly\n",
    "    h1 = h # size of hidden layer1\n",
    "    #h2 = 100 # size of hidden layer2\n",
    "    np.random.seed(1234)\n",
    "    W1 = 0.01 * np.random.randn(D,h1)\n",
    "    b1 = np.zeros((1,h1))\n",
    "    W2 = 0.01 * np.random.randn(D,h1)\n",
    "    b2 = np.zeros((1,h1))\n",
    "    W3 = 0.01 * np.random.randn(h1,K)\n",
    "    b3 = np.zeros((1,K))\n",
    "\n",
    "    num_examples = X.shape[0]\n",
    "\n",
    "    # gradient descent loop\n",
    "    for i in range(no_iter):\n",
    "\n",
    "        # evaluate class scores, [N x K]\n",
    "        #hidden_layer1 = np.maximum(0, np.dot(X, W1) + b1) # note, ReLU activation\n",
    "        #hidden_layer2 = np.maximum(0, np.dot(X, W2) + b2) # note, ReLU activation\n",
    "        hidden_layer = np.maximum(np.dot(X, W1) + b1, np.dot(X, W2) + b2) # note, ReLU activation\n",
    "        scores = np.dot(hidden_layer, W3) + b3\n",
    "\n",
    "        # compute the class probabilities\n",
    "        exp_scores = np.exp(scores)\n",
    "        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True) # [N x K]\n",
    "\n",
    "        # compute the loss: average cross-entropy loss and regularization\n",
    "        corect_logprobs = -np.log(probs[range(num_examples),y])\n",
    "        data_loss = np.sum(corect_logprobs)/num_examples\n",
    "        reg_loss = 0.5*reg*np.sum(W1*W1) + 0.5*reg*np.sum(W2*W2) + 0.5*reg*np.sum(W3*W3)\n",
    "        loss = data_loss + reg_loss\n",
    "        #if i % 1000 == 0:\n",
    "            #print \"iteration %d: loss %f\" % (i, loss)\n",
    "\n",
    "        # compute the gradient on scores\n",
    "        dscores = probs\n",
    "        dscores[range(num_examples),y] -= 1\n",
    "        dscores /= num_examples\n",
    "        \n",
    "        dW3 = np.dot(hidden_layer.T, dscores)\n",
    "        db3 = np.sum(dscores, axis=0, keepdims=True)\n",
    "        \n",
    "        #dhidden[hidden_layer <= 0] = 0\n",
    "        dhidden2 = np.where(np.dot(X, W2) + b2 >= np.dot(X, W1) + b1, np.dot(dscores, W3.T), 0)\n",
    "        dhidden  = np.where(np.dot(X, W2) + b2 <= np.dot(X, W1) + b1, np.dot(dscores, W3.T), 0)\n",
    "  \n",
    "        # finally into W,b, W2,b2\n",
    "  \n",
    "        dW2 = np.dot(X.T, dhidden2)\n",
    "        db2 = np.sum(dhidden2, axis=0, keepdims=True)\n",
    "        dW1 = np.dot(X.T, dhidden)\n",
    "        db1 = np.sum(dhidden, axis=0, keepdims=True)\n",
    "        \n",
    "\n",
    "        # add regularization gradient contribution\n",
    "        dW3 += reg * W3\n",
    "        dW2 += reg * W2\n",
    "        dW1 += reg * W1\n",
    "\n",
    "        # perform a parameter update\n",
    "        W1 += -step_size * dW1\n",
    "        b1 += -step_size * db1\n",
    "        W2 += -step_size * dW2\n",
    "        b2 += -step_size * db2\n",
    "        W3 += -step_size * dW3\n",
    "        b3 += -step_size * db3\n",
    "    return(W1,b1,W2,b2,W3,b3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Hidden Units: 50.00\n",
      "train accuracy : 95.83\n",
      "Test accuracy: 93.33\n",
      "Number of Hidden Units: 100.00\n",
      "train accuracy : 94.17\n",
      "Test accuracy: 95.00\n",
      "Number of Hidden Units: 150.00\n",
      "train accuracy : 95.42\n",
      "Test accuracy: 93.33\n"
     ]
    }
   ],
   "source": [
    "#Question 5.4.3\n",
    "giter=[50,100,150]\n",
    "for j in giter[:]:\n",
    "    W1,b1,W2,b2,W3,b3 = ffnmo(data_X_train,data_Y_train, 0.7,0.0005,1000,j)\n",
    "    hidden_layer = np.maximum(np.dot(data_X_train, W1) + b1, np.dot(data_X_train, W2) + b2) # note, ReLU activation\n",
    "    scores = np.dot(hidden_layer, W3) + b3\n",
    "\n",
    "    predicted_class = np.argmax(scores, axis=1)\n",
    "    print('Number of Hidden Units: %.2f' % j)\n",
    "    print ('train accuracy : %.2f' % (np.mean(predicted_class == data_Y_train)*100))\n",
    "\n",
    "    hidden_layer = np.maximum(np.dot(data_X_test, W1) + b1, np.dot(data_X_test, W2) + b2) # note, ReLU activation\n",
    "    scores = np.dot(hidden_layer, W3) + b3\n",
    "    predicted_class = np.argmax(scores, axis=1)\n",
    "    Test_Acc=(np.mean(predicted_class == data_Y_test)*100)\n",
    "    print ('Test accuracy: %.2f' % Test_Acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
